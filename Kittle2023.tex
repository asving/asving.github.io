
\subsection{Entropy Increase}

\subsubsection{Regular Conditional Distribution}

Given two measurable spaces $(\Omega_1, \mathscr{A}_1)$ and $(\Omega_2, \mathscr{A}_2)$, recall that a Markov kernel on $(\Omega_1, \mathscr{A}_1)$ and $(\Omega_2, \mathscr{A}_2)$ is a map $\kappa : \Omega_1 \times \mathscr{A}_2 \to [0,1]$ if for any $A_2 \in \mathscr{A_2}$, the map $\kappa(\cdot, A_2)$ is $\mathscr{A}_1$-measurable and for any $\omega_1$ the map $A_2 \to \kappa(\omega_1, A_2)$ is a probability measure. 

\begin{definition}
    Let $(\Omega, \mathscr{F}, \mathbb{P})$ be a probability space and let $\mathscr{A} \subset \mathscr{F}$ be a $\sigma$-algebra. Let $(E, \xi)$ be a measurable space and let $Y : (\Omega, \mathscr{F}) \to (E, \xi)$ be a random variable. Then we say that a Markov kernel $$\kappa_{Y, \mathscr{A}} : \Omega \times \xi \to [0,1]$$ on $(\Omega, \mathscr{F})$ and $(E, \xi)$ is a \textbf{regular conditional distribution} if $$\kappa_{Y, \mathscr{A}}(\omega, B) = \mathbb{P}[Y \in B \,|\, \mathscr{A}](\omega) = \mathbb{E}[1_{Y^{-1}(B)}\,|\, \mathscr{A}](\omega).$$ In other words, $$\mathbb{E}[\kappa(\cdot, \mathscr{A})1_A] = \mathbb{P}[A \cap \{ Y \in B \}]$$ for all $A \in \mathscr{A}$.
\end{definition}

We recall that a regular conditional distribution exists whenever $(\Omega, \mathscr{F}, \mathbb{P})$ is a standard probability space. We denote by $(Y|\mathscr{A})$ the regular conditional distribution. 

Given $g_0 \in G$ and a random variable $g$ on $G$ we define $$\mathrm{V}_{g_0}[g] = \mathrm{Var}(\log(g^{-1}g)),$$ whenever $\log(g^{-1}g)$ is defined. 

\begin{lemma}
    Let $\eps > 0$ be sufficiently small and let $g$ and $h$ be independent random variables taking values in $G$. Suppose that the image of $g$ is contained in $B_{\eps}$ and the image of $h$ is contained in $B_{\eps}(h_0)$ for some $h_0 \in G$. Then $$\mathrm{V}_{h_0}(hg) = \mathrm{V}_{h_0}(h) + \mathrm{V}_e(g) + O(\eps^3).$$
\end{lemma}

\begin{proof}
    Let $X = \log(h_0^{-1}h)$ and let $Y = \log(g)$. Then $|X|, |Y| \leq \eps$ almost surely and by Taylor's theorem there is a random variable $E$ with $|E| \ll \eps^2$ almost surely such that $$\log(\epx(X)\exp(Y)) = X + Y + E.$$ Therefore 
    \begin{align*}
        \mathrm{V}_{h_0}(hg) &= E[|X + Y + E|^2] - |E[X + Y + E]|^2 \\
        &=  E[|X + Y|^2] - |E[X + Y]|^2 \\ &+ 2 \mathbb{E}[(X + Y) \cdot E] + \mathbb{E}[|E|^2] - 2\mathbb{E}[X + Y]\mathbb{E}[E] - |\mathbb{E}[E]|^2\\
        &= \mathrm{Var}[X + Y] + O(\eps^3) =  \mathrm{Var}[X] + \mathrm{Var}[Y] + O(\eps^3)
    \end{align*}
\end{proof}

\begin{definition}
    Given some random variable $g$ taking values in $G$, some $\sigma$-algebra $\mathscr{A}$ and some $\mathscr{A}$-measurable random variable $g_0$ taking values on $G$, then we denote by $V_{g_0}(g\,|\, \mathscr{A})$ the $\mathscr{A}$-measurable function $$\mathrm{V}_{g_0}(g\,|\, \mathscr{A})(\omega) = \mathrm{V}_{g_0(\omega)}((g\,|\, \mathscr{A})(\omega, \cdot)).$$
\end{definition}

\begin{lemma}
    Let $\eps > 0$ be sufficiently small and let $a,b$ be random variables and $\mathscr{A}$ a $\sigma$-algebra. Suppose that $b$ is independent from $a$ and $\mathscr{A}$ and let $g_0$ be an $\mathscr{A}$-measurable random variable. Suppose that $g^{-1}a$ and $b$ are almost surely contained in $B_{\eps}$. Then $$\mathrm{V}_{g_0}(ab|\mathscr{A}) = \mathrm{V}_{g_0}(a|\mathscr{A}) + \mathrm{V}_{e}(b) + O(\eps^3).$$
\end{lemma}

\begin{proof}
    We have that $(ab|\mathscr{A}) = (a|\mathscr{A})(b|\mathscr{A}) = (a|\mathscr{A})b$ and so we are done.
\end{proof}

Given an absolutely continuous random variable $Y = f(g) \, d\Haarof{G}$ on $G$ we define $$H(Y) = - \int f \log f \, d\Haarof{G}.$$

\begin{proposition}
    Let $\eps > 0$ be sufficiently small and suppose that $g$ is a random variable taking values in $G$ such that $g_0^{-1}g$ is almost surely in $B_{\eps}$. Then $$H(g) \leq \log \mathrm{V}_{g_0}(g) + C_d  + O(\eps).$$
\end{proposition}


We define $$H(X_2|X_1) = H(X_1,X_2) - H(H_1)$$ and $$H((Y\,|\, \mathscr{A}))(\omega) = H((Y\,|\, \mathscr{A})(\omega, \cdot)).$$ and $$H(X_1|X_2) = \mathbb{E}[H((X_1\,|\, X_2))].$$

\begin{lemma}
    Let $g, s_1, s_2$ be random variables taking values in $G$. Assume that $s_1$ and $s_2$ are absolutely continuous with finite differential entropy and assume that $gs_1$ and $gs_2$ also have finite differential entropy. Write $$k = (H(gs_1) - H(s_1)) - (H(gs_2) - H(s_2)).$$ Then $$H(gs_1|gs_2) \geq k + H(s_1).$$
\end{lemma}

\begin{proof}
    Note that $$H(gs_2|gs_2) = H(gs_1,gs_2) - H(gs_2)$$
\end{proof}

\subsection{Variance Increase}

Throughout denote $G = \mathrm{PSL}_2(\R)$. Given $g_0 \in G$ and a random variable $g$ on $G$ we define $$\mathrm{V}_{g_0}(g) = \mathrm{Var}(\log(g^{-1}g)).$$

\begin{definition}
    Let $g$ be a random variable taking values on $\mathrm{PSL}_2(\R)$ and let $r > 0$. We then define $v(g;r)$ to be the supremum of all $v \geq 0$ such that there exists some $\sigma$-algebra $\mathscr{A}$ and some $\mathscr{A}$-measurable random variable $a$ on $G$ such that $|\log a^{-1}g| \leq r$ almost surely and $$\mathbb{E}[V_{a}[g\,|\, \mathscr{A}]] \geq v r^2.$$
\end{definition}


\begin{proposition}
    There is some absolute constant $c > 0$ such that the following is true
\end{proposition}

\subsection{Typos/Comments}

\begin{enumerate}
    \item Statement of Proposition 1.24: Naive question: Is $c$ really absolute?
    \item Page 33: Lemma 4.1: You can drop the $\mathbb{E}$ and the second bracket inside.
    \item Page 33: Statement of Lemma 4.2: Should $g$ be absolutely continuous?
    \item Page 34: Typo in definition of Markov kernel
    \item Page 35: Proof of Lemma 4.3: I agree that the main claim that $[ab|\mathscr{A}] = [a|\mathscr{A}] [b|\mathscr{A}]$ is intuitive, but I can't quite prove it rigorously. 
    \item Page 36: Is the notation for $KL$ divergence standard? One could also use $D_{\mathrm{KL}}(\nu, \lambda)$.
    \item Page 37: Lemma 4.19: $X$ should be an absolutely continuous random variable.
    \item Page 39: Definition of conditional entropy wrong: $H(X_2)$ instead of $H(X_1)$.
    \item Page 40: Why does the first inequality hold? I don't understand it.
    \item Page 40: Do you want to assume independence in Lemma 4.1? (You do that in Theorem 1.26)
    \item Page 40: Do you want to assume in Theorem 1.26 that the random variables are absolutely continuous?
    \item Page 41: Second paragraph: Write $q(g;r)$ instead of $v(q_{\tau}; \tilde{r})$.
    \item Page 41: Third paragraph: with instead of wish
    \item Page 46: lemma 5.11 Suppose has apostrophe
\end{enumerate}