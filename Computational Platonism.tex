\documentclass{article}
% \usepackage[margin=1.5cm]{geometry}
\usepackage{amsfonts,amssymb,amsmath,amscd}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage[backend=biber]{biblatex}
\addbibresource{AMSNotices.bib}

\titleformat*{\section}{\large\bfseries}

\title{Computational Platonism}

\author{Asvin G\footnote{
This project was supported by a grant from the Simons Foundation International [MPS-SIM-00001691, JT].}.}


\begin{document}
\maketitle

\begin{quote}
\centering
 \textit{The product of mathematics is clarity and understanding. Not theorems, by themselves.}
 
 \raggedleft{Thurston}
\end{quote}

Many mathematicians will agree with Thurston but without being able to specify \textit{what} they are understanding. In other scientific fields, this question is somewhat easier to answer: physicists explain matter, biologists study \textit{life} while astronomers explain the \textit{night sky}. What can we mathematicians point to? Thurston himself \cite{thurston1994proofprogressmathematics} pointed to "formal patterns”, while Davis and Hirsch \cite{davis2012mathematical} spoke of "mental objects with reproducible properties".

In this essay, I propose that mathematicians explain patterns in \textit{computational traces} by synthesizing general axiomatic frameworks that enable deductive proofs of interesting phenomena. All three terms - \textit{explaining}, \textit{patterns} and \textit{computational traces} - will be made more precise in what follows. This point of view will both clarify the relationship of mathematics with the other sciences and single out what makes it different.

\section*{\centering{Computational Traces}}

Mathematics stands out in its precision. Mathematical operations - calculations, symbolic manipulations, parsing proofs and algorithms - can all be carried out by anyone who understands the basic rules. Here we are only speaking of the act of following mathematics; the act of creating it is not algorithmic in any clear way.

Turing's formalization of computation \cite{turing1936computable} was one of the landmark achievements of early twentieth century mathematics and philosophy. His simple machine can carry out any algorithmic procedure when fed appropriate rules and inputs. Over the past century, we have realized the universality of computation: both in the substrates on which it can be carried out (the human mind, electronic circuits, quantum circuits \dots), as well as the processes which can be modeled by this formalization (the Church-Turing thesis posits that \textit{all} phenomena can be modeled so).

I will argue shortly for mathematics as the study of a particular class of computations, but it is also important to ground computational physically - they provide the ultimate source of verification within mathematics. We will treat a \textit{computation} as a specification of a class of physical processes (defined by Turing). Regardless of whether Galileo's experiment of dropping two balls is carried out inside or outside, in London or Mumbai, we expect similar outcomes. Similarly, we expect the content of a proof to be independent of where and how it is read (by humans or by automated machines). This notion is very general and includes numerical computations, symbolic manipulations or even parsing proofs. 

The kind of computations mathematicians care about is best encapsulated by \textit{definitions}. A definition specifies an "object", represented by a string, and a class of allowed operations, represented by string rewrite rules, to obtain the output. For example, the natural numbers are commonly defined using an initial object $\underline{0}$ and operations $S,+,\times$ satisfying certain properties. The $n$-th natural number is the outcome of applying $S$ to $\underline{0}$ $n$ times: $\underline{n} = S^n\underline{0}$ while the operations $+,\times$ are defined recursively by
\begin{enumerate}
    \item $\underline{n} + \underline{0} = \underline{n}$
    \item $\underline{n} + S\underline{m} = S\underline{n+m}$
    \item $\underline{n} \times \underline{0} = \underline{0}$
    \item $\underline{n} \times S\underline{m} = \underline{n} + \underline{n}\times \underline{m}$.
\end{enumerate}

These rules allow us to compute the outcomes of $S,+,\times$ on any input numbers and provide a way to interact with this computational artifact, i.e., it is an \textit{action-protocol interface.} Similarly, a proof is a specification of how to parse a sequence of strings and either accept or reject its validity.

\textit{Computational traces} are the symbolic outputs of such processes: a table of prime numbers, an algorithm for checking primality, an ansatz for solving differential equations or even a pictorial or algebraic proof that the Pythagorean theorem holds for right-angled triangles in the plane. Each comes with protocols for interpretation, with the promise that the protocol is completely sufficient to verify their validity.

At this level, mathematics is completely constructive, unambiguous and pre-axiomatic. We learn to add numbers before we have any idea of induction, let alone the Peano axioms. From this point of view, an axiomatic system is merely the specification of certain rules that let us parse proofs expressed in that language, and so specifies a class of possible computations. I will return to the role of axiomatic systems shortly.

\section*{\centering{Patterns, or What Is Surprising}}

The bread and butter of practicing mathematics is to identify interesting and surprising regularities in the computational traces we generate, whether we are interested in formulating new conjectures, proving said conjectures, or finding new definitions and axiomatic frameworks in which to understand these proofs.

A young Gauss conjectured in 1792-93 that the proportion of prime numbers around a large integer $X$ is roughly $1/\log X$ based on prime number tables. Providing a rigorous justification would take another century, but the genesis lay in simple computation. While this example is the simplest instance of "patterns in computational traces" leading to deep mathematics, it is far from the only one.

Another common pattern emerges when computations lead to the correct answer for unknown reasons (as often happens in physics). Fourier solved the heat equation in 1822 by assuming that every function could be expressed as a trigonometric series. This was completely unjustified at the time but it led to brilliant, verifiable solutions, and a deep theory justifying the equations that we are still exploring. 

Yet another source comes from examining and simplifying rigorous but complicated mathematics. Extracting the kernel of a proof and basing new frameworks on these essential arguments has proven immensely profitable. Laurent Schwartz's theory of distributions provided a new framework that generalized the concept of a function, allowing derivatives to be defined for new objects in a way that preserved essential tools like integration by parts. Similarly, differential-geometric methods entered commutative algebra through a similar generalization.

In all these examples, computational traces play a crucial role in the genesis of deep mathematics. While "surprising patterns" are subjective and hard to pin down, their importance is undeniable. They provide the lifeblood for new mathematics, and the verification of these computational traces grounds mathematical truth in a physical process.


\section*{\centering{Existence and Explanation}}

Mathematicians often operate as if \textit{mathematical objects} had a platonic existence. What might this mean? To clarify this, consider how other objects exist: a table, an electron or  DNA. Their existence is mediated entirely through our methods of interaction. We posit their existence in order to compress and explain a mass of observed phenomena.

A table as we perceive it (as solid, brown and smooth to the touch) is not meaningful in terms of the physics of electrons and atoms. Yet, this description serves our everyday purposes (but not all purposes!) much better than a description in atomic terms. Similarly, an electron in modern physics is a mathematical postulate whose existence compresses vast quantities of observational data from biology, chemistry and astronomy. Similarly, Schr\"odinger postulated in 1944 \cite{schrodinger2012life} the existence of an "aperiodic crystal" - a molecule with a stable, ordered, but non-repeating structure - that would hold the genetic code and transmit it across generations. This was prior to the physical isolation of DNA which would be carried out in the following decade and confirm Schr\"odinger's prediction. The existence of DNA was essentially forced in trying to understand genetic inheritance across living organisms in the context of the cell. 

The question of \textit{existence} dissolves then into questions about \textit{interactive protocols} and \textit{useful compressions}. What is \textit{real} depends on what needs explaining as well as the conceptual framework of the explainer.

In physics, we explain our observations by postulating objects in mathematical frameworks (an electron, a quantum wavefunction, the metric tensor in general relativity \dots) as well as specifying a correspondence between computational outcomes and physical observations. The mathematical objects need not directly correspond to observation (the metric tensor is unmeasurable because of the general covariance of the theory and the quantum wavefunction cannot be directly measured given the Born rule), and the mathematical objects do not have to be rigorously grounded in mathematics. The final arbiter of correctness is correspondence with physical observation.


Analogously, mathematicians believe in the existence of \textit{mathematical objects} because they explain masses of computational traces, the analogue of physical experiment. Just as the theory of the electron explains disparate physical phenomena in a self-consistent way, the theory of the infinite explains disparate computational phenomena. In Peano arithmetic, we assert the existence of an infinite set $\mathbb N$ containing all natural numbers as well as an inductive property that all sets $K \subset \mathbb N$ must satisfy:
\[ \text{If } \underline{0} \in K \text{ and for any } \underline{n}, \underline{n} \in K \implies S\underline{n} \in K; \text{ then } K = \mathbb N.  \]
Even though we have not directly verified this property on all possible sets $K$, we believe in this framework because it explains finitary computational traces that we can verify directly. Infinite sets are not just a useful explanation in number theory however, they provide clarifying conceptual explanations in essentially every field of modern mathematics. Despite the existence of infinite sets being completely unverifiable from the perspective of finitary computations, their existence has been encoded directly into the fundamental axiomatic framework of mathematics, ZFC. This is the central claim of this essay:

\textbf{Claim:} \textit{An axiomatic framework in mathematics plays exactly the role of a mathematical theory in physics. One explains computational experiments, the other physical experiments.}

Proving a statement within an axiomatic framework parallels deriving molecular dynamics from quantum field theory. Proving the Riemann hypothesis is as much a test of our current axiomatic frameworks as of our ingenuity. We believe in the Riemann Hypothesis because of our mass of computational traces - direct zero computations, analogical evidence from finite fields and spectral interpretations connecting to random matrix theory among others. We demand of our proofs not just that they certify truth but that they \textit{explain} the patterns we have seen. 


The analogy runs deeper: we might find contradictions in any sufficiently powerful axiomatic framework, such as Peano Arithmetic. Such contradictions are a failure of our axiomatic framework to faithfully describe the computational objects we have defined, exactly as a physical theory's failure corresponds to its inability to faithfully describe the physical observations we make. The Peano Axioms are conjectural statements that we expect the computational procedure ($\underline{0},S,+,\times$) to satisfy and a contradiction points to an axiom that fails at some point - just as Newton's theory failed to describe the perihelion of Mercury.

Mathematicians can even sense an object’s ”presence” before formalizing it.
The hypothetical field with one element is meant to explain strong analogies
between number theory, Riemann surfaces, and curves over finite fields. With-
out a candidate formalization, this heuristic has guided modern mathematics
including Scholze’s perfectoid geometry and matroid theory. Our formalizations try to make precise our intuitions.

However, G\"odel's incompleteness theorem guarantees that even a perfectly faithful description will necessarily be \textit{incomplete}: there will always be observable patterns that can't be deduced within the existing framework. That we can know such a result points to the self-reference at the heart of mathematics - where we model computational traces in terms of axiomatic frameworks given in computational terms. 

This self-reference also lets us explore new mathematics once we have defined a new axiomatic (and implicitly computational) framework, observe new patterns in these computational traces and then build new frameworks ad-infinitum. Mathematics provides its own fuel for its progress.

\section*{\centering{Looking to the future}}

So far, I have described existing mathematical practice and identified the experimental and theoretical strains within mathematics. Let us look at the future now. It seems plausible that theorem-proving might be automated very soon, perhaps in a formal language like Lean. In the near future, a computer system will almost certainly be able to prove simple new lemmas and theorems.

This will not be the first time technology affects mathematics. Before the rise of computers, virtuosity with computation was seen as a virtue. Today, we outsource such computations to our computers and focus on "insight" - perhaps to our detriment. Similarly, I suspect that mathematicians will outsource the details of proofs to computers and concentrate instead on "insight", focusing on the conceptual structure of mathematics, the axiomatic framework and proof \textit{strategies} that they enable. I will end with two thought experiments.

\begin{enumerate}
    \item Suppose an automated theorem prover outputs a billion line long proof of the Riemann Hypothesis in Lean. How will we react?
    \item Suppose an automated theorem prover outputs a billion line long contradiction in Peano Arithmetic. How will we react?
\end{enumerate}



\printbibliography

\end{document}